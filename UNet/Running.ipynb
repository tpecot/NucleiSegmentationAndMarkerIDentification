{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with U-Net\n",
    "\n",
    "The first step is to load the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import tifffile as tiff\n",
    "\n",
    "from utils import getfiles, get_image, run_models_on_directory, get_image_sizes\n",
    "from models import unet as unet\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import skimage as sk\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imsave\n",
    "from skimage import exposure\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folders where the classifier, the images to run and the results to come are located as well as the classifier name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image  1  of  3\n",
      "Processing image  2  of  3\n",
      "Processing image  3  of  3\n"
     ]
    }
   ],
   "source": [
    "trained_classifier_directory = \"./trainedClassifiers/nucleiSegmentation/\"\n",
    "\n",
    "nb_channels = 1\n",
    "nb_classes = 3\n",
    "imaging_field_x = 256\n",
    "imaging_field_y = 256\n",
    "normalizing_window_size_x = 64\n",
    "normalizing_window_size_y = 64\n",
    "\n",
    "\n",
    "model_prefix = \"Unet_CFtraining_DA100_10ep\"\n",
    "direc_name = './datasets/nucleiSegmentation_E2Fs/Confocal_test/'\n",
    "data_location = os.path.join(direc_name, 'Images')\n",
    "classes_location = os.path.join(direc_name, 'Unet_CF_DA100_10ep')\n",
    "\n",
    "model_weights = os.path.join(trained_classifier_directory,  model_prefix + \".h5\")\n",
    "model = unet(nb_classes, imaging_field_x, imaging_field_y, nb_channels, weights_path = model_weights)\n",
    "model.load_weights(model_weights)\n",
    "predictions = run_models_on_directory(data_location, classes_location, model = model, \n",
    "                                      normalizing_window_size_x = normalizing_window_size_x, \n",
    "                                      normalizing_window_size_y = normalizing_window_size_y)\n",
    "\n",
    "model_prefix = \"Unet_CFWFtraining_DA100_10ep\"\n",
    "classes_location = os.path.join(direc_name, 'Unet_CFWF_DA100_10ep_CF')\n",
    "\n",
    "model_weights = os.path.join(trained_classifier_directory,  model_prefix + \".h5\")\n",
    "model = unet(nb_classes, imaging_field_x, imaging_field_y, nb_channels, weights_path = model_weights)\n",
    "model.load_weights(model_weights)\n",
    "predictions = run_models_on_directory(data_location, classes_location, model = model, \n",
    "                                      normalizing_window_size_x = normalizing_window_size_x, \n",
    "                                      normalizing_window_size_y = normalizing_window_size_y)\n",
    "\n",
    "\n",
    "model_prefix = \"Unet_WFtraining_DA100_10ep\"\n",
    "direc_name = './datasets/nucleiSegmentation_E2Fs/Widefield_test/'\n",
    "data_location = os.path.join(direc_name, 'Images')\n",
    "classes_location = os.path.join(direc_name, 'Unet_WF_DA100_10ep')\n",
    "\n",
    "model_weights = os.path.join(trained_classifier_directory,  model_prefix + \".h5\")\n",
    "model = unet(nb_classes, imaging_field_x, imaging_field_y, nb_channels, weights_path = model_weights)\n",
    "model.load_weights(model_weights)\n",
    "predictions = run_models_on_directory(data_location, classes_location, model = model, \n",
    "                                      normalizing_window_size_x = normalizing_window_size_x, \n",
    "                                      normalizing_window_size_y = normalizing_window_size_y)\n",
    "\n",
    "model_prefix = \"Unet_CFWFtraining_DA100_10ep\"\n",
    "classes_location = os.path.join(direc_name, 'Unet_CFWF_DA100_10ep_WF')\n",
    "\n",
    "model_weights = os.path.join(trained_classifier_directory,  model_prefix + \".h5\")\n",
    "model = unet(nb_classes, imaging_field_x, imaging_field_y, nb_channels, weights_path = model_weights)\n",
    "model.load_weights(model_weights)\n",
    "predictions = run_models_on_directory(data_location, classes_location, model = model, \n",
    "                                      normalizing_window_size_x = normalizing_window_size_x, \n",
    "                                      normalizing_window_size_y = normalizing_window_size_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
